{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code used for implementing the classifer in paper Tissue Classification with Gene Expression Profiles.\n",
    "\n",
    "**Data Set**: Raw/original data used in the Golub etc. paper. \n",
    "    - Train Data: 38 bone marrow samples(27ALL, 11AML).\n",
    "    - Test Data: 34 samples(24 bone marrow, 10 peripheral blood samples, 20ALL, 14AML).\n",
    "    - Predictors: 7129 gene expression levels represent 6817 genes.\n",
    "    \n",
    "**Main Purpose**: Multiple classifications: NN, SVM, AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce using R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Load and transform dataset. \n",
    "Install Bioconductor biocLite package in order to access the golubEsets library. [golubEsets](https://bioconductor.org/packages/release/data/experiment/manuals/golubEsets/man/golubEsets.pdf) contains the raw data used by Todd Golub in the original paper.\n",
    "\n",
    "Load the training, testing data from library golubEsets. Also transpose the data to make observations as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Train</th><td>38  </td><td>7129</td></tr>\n",
       "\t<tr><th scope=row>Test</th><td>34  </td><td>7129</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "\tTrain & 38   & 7129\\\\\n",
       "\tTest & 34   & 7129\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Train | 38   | 7129 | \n",
       "| Test | 34   | 7129 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      [,1] [,2]\n",
       "Train 38   7129\n",
       "Test  34   7129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Train</th><th scope=col>Test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>ALL</th><td>27</td><td>20</td></tr>\n",
       "\t<tr><th scope=row>AML</th><td>11</td><td>14</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Train & Test\\\\\n",
       "\\hline\n",
       "\tALL & 27 & 20\\\\\n",
       "\tAML & 11 & 14\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Train | Test | \n",
       "|---|---|\n",
       "| ALL | 27 | 20 | \n",
       "| AML | 11 | 14 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Train Test\n",
       "ALL 27    20  \n",
       "AML 11    14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The code below is commented out since it is unnecessary and time-consuming to run it everytime. Run it if needed.\n",
    "# options(repos='http://cran.rstudio.com/') \n",
    "# source(\"http://bioconductor.org/biocLite.R\")\n",
    "# biocLite(\"golubEsets\")\n",
    "suppressMessages(library(golubEsets))\n",
    "#Training data\n",
    "data(Golub_Train)\n",
    "golub_train_p = t(exprs(Golub_Train))\n",
    "golub_train_r =pData(Golub_Train)[, \"ALL.AML\"]\n",
    "golub_train_l = ifelse(golub_train_r == \"AML\", 1, 0)\n",
    "\n",
    "#Testing data\n",
    "data(Golub_Test)\n",
    "golub_test_p = t(exprs(Golub_Test))\n",
    "golub_test_r = pData(Golub_Test)[, \"ALL.AML\"]\n",
    "golub_test_l = ifelse(golub_test_r == \"AML\", 1, 0)\n",
    "\n",
    "#Show summary\n",
    "rbind(Train = dim(golub_train_p), Test = dim(golub_test_p))\n",
    "cbind(Train = table(golub_train_r),Test = table(golub_test_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Feature selection using TNoM score\n",
    "\n",
    "As a bootstrap is used we set a seed to ensure reproducibility of this reproduce work. We follow the steps in section 4 of the paper and it may take some time to run the bootstrap. We select genes with TNoM score less than 14 and also have bootstrap p-value less than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set.seed(201703)\n",
    "# TNoM score\n",
    "r_score = function(slabel){\n",
    "    total = length(slabel)\n",
    "    n = sum(slabel == 0) \n",
    "    p = sum(slabel == 1) \n",
    "    temp = min(n, p)\n",
    "    d = ifelse(n < p, 1, -1)\n",
    "    for(i in 1:(total-1)){\n",
    "        count = sum(slabel[1:i] == 0)\n",
    "        t_t = min(n-2*count+i, p+2*count-i)\n",
    "        t_d = ifelse((n-2*count+i)<(p+2*count-i),1,-1)\n",
    "        if(t_t < temp){\n",
    "            temp = t_t\n",
    "            d = t_d\n",
    "        }\n",
    "    }\n",
    "    c(temp, t_d)\n",
    "}\n",
    "\n",
    "# Significance (using bootstrap with size 1000 to replace)\n",
    "r_bootstrap = function(gene, label){\n",
    "    total = length(label)\n",
    "    index = order(gene)\n",
    "    s_l = label[index]\n",
    "    score= r_score(s_l)\n",
    "    dist_score = numeric(200)\n",
    "    for(i in 1:200){\n",
    "        temp = sample(1:total)\n",
    "        t = r_score(label[temp])\n",
    "        dist_score[i] = t[1]\n",
    "    }\n",
    "    prob = mean(dist_score<score[1])\n",
    "    c(score[1], score[2], prob)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform the caculation, this may take a while since the inevitable loops in above functions\n",
    "a = apply(golub_train_p, 2, r_bootstrap, label = golub_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select informative genes and subset the train and test data\n",
    "index = (1:7129)[a[1,]<14 & a[3,]<0.01]\n",
    "b = order(a[1,index])[1:50]\n",
    "\n",
    "# data subsetting\n",
    "train_cl = golub_train_p[, index]\n",
    "test_cl = golub_test_p[, index]\n",
    "train_paper3 = train_cl[, b]\n",
    "train_response = golub_train_r\n",
    "test_paper3 = test_cl[, b]\n",
    "test_response = golub_test_r\n",
    "save(train_paper3, train_response, test_paper3, test_response, file = \"paper3.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Step 3(a)**_ Nearest Neighbor Classification\n",
    "\n",
    "Pearson correlation is used as the measure of distance in the nearest neighbor classification. In the paper, they have 91.6% classification rate while we 33/34 = 97% classification rate on test data as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Train_Actual\n",
       "Train_Predict ALL AML\n",
       "          ALL  27   0\n",
       "          AML   0  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "            Test_Actual\n",
       "Test_Predict ALL AML\n",
       "         ALL  20   1\n",
       "         AML   0  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the classifier\n",
    "cl_nn = function(new_s, train, train_label){\n",
    "    # use Pearson correlation\n",
    "    corr = apply(train, 1, cor, new_s)\n",
    "    train_label[corr==max(corr)]\n",
    "}\n",
    "\n",
    "# prediction\n",
    "nn_train_pr = apply(train_cl,1, cl_nn, train_cl, golub_train_r)\n",
    "nn_test_pr = apply(test_cl,1, cl_nn, train_cl, golub_train_r)\n",
    "\n",
    "# show result of prediction\n",
    "table(Train_Predict = nn_train_pr, Train_Actual = golub_train_r)\n",
    "table(Test_Predict = nn_test_pr, Test_Actual = golub_test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Step 3(b)**_ SVM\n",
    "\n",
    "Use R package e1701, which has svm already implemented. As we use the function in e1701, we don't have unclassified observations as in the paper. They have accuracy rate of 93.3% for linear kernel and 94.4% for quadratic kernel. We have 31/34 = 91.2% correctly classified by linear kernel and 32/34 = 94.1% correctly classified by quadratic kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The next two lines are commented out. If you don't have the packages in your env, you need to run them first.\n",
    "#options(repos='http://cran.rstudio.com/') \n",
    "#install.packages(\"e1071\")\n",
    "suppressMessages(library(e1071))\n",
    "# need to set.seed for reproducibility\n",
    "set.seed(201702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Train_Actual\n",
       "Train_Predicted  0  1\n",
       "              0 27  0\n",
       "              1  0 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "              Test_Actual\n",
       "Test_Predicted  0  1\n",
       "             0 16  2\n",
       "             1  4 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the data for R functions\n",
    "r_train = data.frame(train_cl, Y = factor(golub_train_l))\n",
    "r_test =data.frame( test_cl, Y = factor(golub_test_l))\n",
    "\n",
    "# build svm with linear kernel\n",
    "svm_linear = svm(Y~., data = r_train)\n",
    "svm_l_trpr = predict(svm_linear, r_train)\n",
    "svm_l_tepr = predict(svm_linear, newdata = r_test)\n",
    "\n",
    "# Result summary\n",
    "table(Train_Predicted  = svm_l_trpr, Train_Actual = golub_train_l)\n",
    "table(Test_Predicted = svm_l_tepr, Test_Actual = golub_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Train_Actual\n",
       "Train_Predicted  0  1\n",
       "              0 27  0\n",
       "              1  0 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "              Test_Actual\n",
       "Test_Predicted  0  1\n",
       "             0 20  3\n",
       "             1  0 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#build svm with quadratic kernel\n",
    "svm_quad = svm(Y~., data = r_train, kernel = \"polynomial\", degree = 2,  gamma =0.01, coef0 = 100)\n",
    "svm_q_trpr = predict(svm_quad, r_train)\n",
    "svm_q_tepr = predict(svm_quad, newdata = r_test)\n",
    "\n",
    "# Result_summary\n",
    "table(Train_Predicted  = svm_q_trpr, Train_Actual = golub_train_l)\n",
    "table(Test_Predicted = svm_q_tepr, Test_Actual = golub_test_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Step 3(c)**_ AdaBoost\n",
    "\n",
    "Use R package fastAdaboost, which use decision trees as weak classifiers as the paper use decision stumps as week learners. They have accuracy rate of 95.8% and we have accuracy rate of 31/34 = 91.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The next two lines are commented out. If you don't have the packages in your env, you need to run them first.\n",
    "# options(repos='http://cran.rstudio.com/') \n",
    "# install.packages(\"fastAdaboost\")\n",
    "suppressMessages(library(fastAdaboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Train_Actual\n",
       "Train_Predict  0  1\n",
       "            0 27  0\n",
       "            1  0 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "            Test_Actual\n",
       "Test_Predict  0  1\n",
       "           0 18  1\n",
       "           1  2 13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the classifier iter 100\n",
    "ada_cl = adaboost(Y~., data = r_train, 100)\n",
    "\n",
    "# prediction and result\n",
    "ada_train_pr = predict(ada_cl, r_train)\n",
    "ada_test_pr = predict(ada_cl, newdata = r_test)\n",
    "table(Train_Predict = ada_train_pr$class, Train_Actual = golub_train_l)\n",
    "table(Test_Predict = ada_test_pr$class, Test_Actual = golub_test_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary of Reproduce result\n",
    "\n",
    "In this notebook, we reproduce paper 3 and have 3 algorithm reproduce, NN, SVM and Adaboost. One thing worth notice is that in the last one, we seem not reproduce the iteration 1000 and 10000 results. Actually, we do. But since they are the same as iteration 100's result, we didn't include them then.\n",
    "\n",
    "Also, in the paper, they use LOOCV while we don't. \n",
    "\n",
    "**Prediction Comparison**:\n",
    "Prediction result comparison is included in each algorithm in step 3.\n",
    "We have about the same accuracy rates for each method used in the paper. Except for random factors such as seeds, there are also two factors that we cannot completely reproduce their result. First, they use leave one out cross validation to calculte the accuracy rate while we don't. Second, we use functions from existing packages instead of reimplement each method strictly as in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
