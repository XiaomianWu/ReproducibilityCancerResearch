{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the code for GeneLogit library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### localFDR_cal.R ############################################\n",
    "#######################################################  \n",
    "    f1 = function(y, lambda, t, freq)\n",
    "    {\n",
    "       k = length(t);\n",
    "       k1 = k-1;\n",
    "\n",
    "       support = cumprod( (1-t[1:k1])^(lambda[1:k1]-lambda[2:k]) );\n",
    "       support = c(1, support);\n",
    "\n",
    "       ss = rep(support, freq);\n",
    "       lam = rep(lambda, freq);\n",
    "\n",
    "       ss*lam*(1-y)^(lam-1);\n",
    "     }\n",
    "\n",
    "    F1 = function(y, lambda, t, freq)\n",
    "    {\n",
    "       lam = rep(lambda, freq);\n",
    "       1 - f1(y, lambda, t, freq)*(1-y)/lam;\n",
    "    }\n",
    "\n",
    "    log.likelihood.tau.total = function(tau, y, z, t, sigma2, freq)\n",
    "    {\n",
    "       k = length(t);\n",
    "       k1 = k-1;\n",
    "\n",
    "       lambda = exp(tau) + 1;\n",
    "       \n",
    "       sigma.local = lambda[2:k]/(lambda[2:k]-1);\n",
    "       sigma.local = 1;\n",
    "       diff = tau[1:(k-1)] - tau[2:k];\n",
    "       diff = diff/sigma.local;\n",
    "       \n",
    "       result = -sum( diff^2 )/2/sigma2;\n",
    "       \n",
    "       result2 = f1(y, lambda, t, freq)[z==1];\n",
    "       result2 = log(result2);\n",
    "\n",
    "       result + sum(result2);\n",
    "    }\n",
    "\n",
    "    sample.tau.one.component = function(i, tau, y, z, t, sigma2, sigma2.mean, freq)\n",
    "    {\n",
    "       alpha.old = log.likelihood.tau.total(tau, y, z, t, sigma2, freq);\n",
    "\n",
    "       tau.single.old = tau[i];\n",
    "       tau.single.new = tau.single.old + rnorm( 1, 0, sqrt(sigma2.mean) );\n",
    "\n",
    "       tau[i] = tau.single.new;\n",
    "       alpha.new = log.likelihood.tau.total(tau, y, z, t, sigma2, freq);\n",
    "                                      \n",
    "       alpha = exp( alpha.new - alpha.old );                 \n",
    "                                        #print(c(alpha, alpha.new, alpha))\n",
    "       if( runif(1) < alpha ) return(tau.single.new)\n",
    "       else return(tau.single.old);\n",
    "    }\n",
    "\n",
    "\n",
    "    location = function(y, t)\n",
    "    {\n",
    "      n = length(y);\n",
    "      ll = numeric(n);\n",
    "\n",
    "      for(i in 1:n) ll[i] = which(t>y[i])[1];\n",
    "\n",
    "      ll <- factor(ll,levels=1:length(t))\n",
    "\n",
    "      ll;\n",
    "     }\n",
    "\n",
    "\n",
    "    localFDR = function(y, k, n.simu, v=.30*k)       #v for prior of sigma2\n",
    "    {\n",
    "       #the y is a vector of pvalues to be decomposed\n",
    "       # t is the vector of kots not including 0 but including 1\n",
    "\n",
    "       y = sort(y);\n",
    "\n",
    "       small = 100*.Machine$double.eps;\n",
    "       y[y<small] = small;\n",
    "       y[y>1-small] = 1-small;\n",
    "\n",
    "       t = comp.t(y, k);\n",
    "       n = length(y);  #number of p-value\n",
    "\n",
    "       zeta = 0.0001;\n",
    "\n",
    "     #initial values\n",
    "\n",
    "       pi0 = mean(y>.5)/.5;\n",
    "\n",
    "       tau = rep(.5, k);\n",
    "       sigma2 = .1;\n",
    "\n",
    "       tau.mean = numeric(k);\n",
    "       pi0.mean = 0;\n",
    "       sigma2.mean = sigma2;\n",
    "\n",
    "       pi0.save = numeric(n.simu);\n",
    "       freq = table( location(y, t) );\n",
    "\n",
    "       lambda = exp(tau) + 1;\n",
    "       k1 = k-1;\n",
    "       \n",
    "       for(i in 1:n.simu)\n",
    "       {\n",
    "          fdr = (1-pi0)*f1(y, lambda, t, freq);\n",
    "          fdr = pi0/(fdr + pi0);\n",
    "          z = rbinom(n, 1, 1-fdr);           ##\n",
    "\n",
    "          sum.z = sum(z);\n",
    "          if(i>100) pi0 = rbeta(1, n-sum.z+1, sum.z+1);  ##\n",
    "          pi0.save[i] = pi0;\n",
    "\n",
    "          for(j in k:1) tau[j] = sample.tau.one.component(j, tau, y, z, t, sigma2, sigma2.mean, freq);      ##\n",
    "\n",
    "          lambda = exp(tau) + 1;\n",
    "          sigma.local = lambda[2:k]/(lambda[2:k]-1);\n",
    "          sigma.local = 1;\n",
    "          diff = tau[1:(k-1)] - tau[2:k];\n",
    "          diff = diff/sigma.local;\n",
    "       \n",
    "          scale = zeta*v + sum( diff^2 );\n",
    "          scale = scale/(v+k-1);\n",
    "\n",
    "          sigma2 = scale*(k-1+v)/rchisq(1, k-1+v);      #print(sigma2); print(tau);\n",
    "    ##################################################\n",
    "          ww = 1/i;\n",
    "          tau.mean = tau.mean*(1-ww) + tau*ww;\n",
    "          pi0.mean = pi0.mean*(1-ww) + pi0*ww;\n",
    "          sigma2.mean = sigma2.mean*(1-ww) + sigma2*ww;\n",
    "          \n",
    "          lambda.mean = exp(tau.mean) + 1;\n",
    "\n",
    "   ######################################################\n",
    "          if(i%%10!=0) next;\n",
    "\n",
    "          fdr = (1-pi0.mean)*f1(y, lambda.mean, t, freq);\n",
    "          fdr = pi0.mean/(fdr + pi0.mean);\n",
    "\n",
    "          FDR = (1-pi0.mean)*F1(y, lambda.mean, t, freq);\n",
    "          FDR = 1 - FDR/(FDR + pi0.mean*y);\n",
    "          #plot(y, f1(y, lambda.mean, t, freq), type=\"l\");\n",
    "          #plot(y, fdr, type=\"l\", xlab='raw pvalue', ylab='local FDR');\n",
    "          #lines(y, FDR, type=\"l\");\n",
    "\n",
    "          #print(i); print(\"posterior of pi0\");\n",
    "          #print(quantile(pi0.save[1:i], c(.025,.50,.975)));\n",
    "      }\n",
    "\n",
    "      F1.value = F1(y, lambda.mean, t, freq);\n",
    "      F = pi0.mean*y + (1-pi0.mean)*F1.value;\n",
    "      NPV = pi0.mean*(1-y)/(1-F);\n",
    "      \n",
    "      f.value = pi0.mean + (1-pi0.mean)*f1(y, lambda.mean, t, freq);\n",
    "      \n",
    "      fdr\n",
    "    }\n",
    "\n",
    "#####################################################################################    \n",
    "comp.t = function(pvalue, k)   #select the knots t based on the quantiles of pavlue\n",
    "{\n",
    "   pvalue = sort(pvalue);\n",
    "   n = length(pvalue);\n",
    "\n",
    "   t=numeric(k);\n",
    "   for(j in 1:k) t[j] = pvalue[n*j/k];\n",
    "   t[k] = 1;\n",
    "\n",
    "   start = which(t==1)[1]-1;\n",
    "   diff = 1 - t[start];\n",
    "   step = diff/(k-start);\n",
    "\n",
    "   for(i in (start+1):k) t[i] = t[start] + (i-start)*step;\n",
    "   t;\n",
    "}\n",
    "    \n",
    "######################################################################################     \n",
    "   raw.pvalues.cal.one = function(x, y)  #pvalues for simple logistic regression\n",
    "    {\n",
    "       n = nrow(x)\n",
    "       p = ncol(x)\n",
    "       \n",
    "       pvalue = array(0, c(p,2)) #the 1st column is the pvalue \n",
    "                                 #the 2dn column is the probability of regression coefficients being positive                 \n",
    "       one = rep(1,n)                         \n",
    "       for(j in 1:p)\n",
    "       {\n",
    "          xx = cbind(one, x[,j])\n",
    "          model = glm.fit(xx, y, family=binomial())       #pvalue based on simple logistic regression    \n",
    "          pvalue[j,1] = 1 - pchisq(model$null.deviance-model$deviance, df=1)\n",
    "          pvalue[j,2] = ifelse(model$coefficients[2]>0, 1-pvalue[j,1]/2, pvalue[j,1]/2)\n",
    "       }\n",
    "       pvalue       \n",
    "    } \n",
    "    \n",
    "    localFDR.cal = function(x, y, k=40, n.simu=10000, v=40)\n",
    "    {\n",
    "      result = raw.pvalues.cal.one(x, y) ; #hist(result[,1])\n",
    "      rank1 = rank( result[,1] )     \n",
    "            \n",
    "      localFDR.fit = localFDR(result[,1], k, n.simu, v)\n",
    "      localFDR.fit = localFDR.fit[rank1]   #keep the original order of raw pvalues\n",
    "      \n",
    "      result = cbind(localFDR.fit, result[,2]) \n",
    "      write.dta(as.data.frame(result), \"localFDR_value.dta\")\n",
    "   }  \n",
    "   \n",
    "################ logistic_penalized.R #############################################\n",
    "#############################################################\n",
    "  solve.special = function(lambda, U, t.V, b)  # A = tau * I      Numerical Recipes chapter 2.7\n",
    "  {\n",
    "     Z = U/lambda;\n",
    "     H = t.V %*% Z;\n",
    "     H[row(H)==col(H)] = H[row(H)==col(H)] + 1;\n",
    "\n",
    "     D = b/lambda;\n",
    "     as.vector(D - Z %*% solve(H, t.V %*% D));\n",
    "  }    \n",
    "\n",
    "  b.update.wide = function(t.x, x, y, b, lambda)  #p>n, more predictors than subjects\n",
    "  {\n",
    "      p = ncol(x);\n",
    "      pi1 = plogis(as.vector(x %*% b));\n",
    "      w = pi1*(1-pi1);\n",
    "\n",
    "      right.side = as.vector( y - pi1 + w * (x %*% b) ); #as.vector( y - pi1 + diag(w)%*%(x %*% b) );\n",
    "      right.side = as.vector(t.x%*%right.side)\n",
    "\n",
    "      u = numeric(p);\n",
    "      u[1] = -lambda;\n",
    "      v = numeric(p);\n",
    "\n",
    "      v[1] = 1;\n",
    "      U = t(x) * rep(sqrt(w), each=p) #t(x) %*% diag(sqrt(w)), matrix of dimension p by n\n",
    "      t.V = rbind(t(U), v);\n",
    "      U = cbind(U, u);\n",
    "\n",
    "      solve.special(lambda, U, t.V, right.side)\n",
    "   }\n",
    "\n",
    "  b.update.high = function(t.x, x, y, b, lambda)  #n>p, more subjects than predictors\n",
    "  {\n",
    "      p = ncol(x);\n",
    "      pi1 = plogis(as.vector(x %*% b));\n",
    "      w = pi1*(1-pi1);\n",
    "                                         \n",
    "      right.side = y - pi1 + as.vector(w * (x %*% b) ); #as.vector( y - pi1 + diag(w)%*%(x %*% b) );    \n",
    "      right.side = as.vector(t.x%*%right.side)\n",
    "                             \n",
    "      Q = diag(p)\n",
    "      Q[1,1] = 0\n",
    "      left.side = (t.x * rep(w, each=p)) %*% x + lambda * Q   ;   \n",
    "      solve(left.side, right.side)\n",
    "   }\n",
    "\n",
    "\n",
    "  logistic.penalized = function(x, y, lambda)\n",
    "\n",
    "  {                                        #n is the number of subjects\n",
    "                                           #p is the number of predictors + intercept                                       \n",
    "     if(lambda < 1.e-5) lambda = 1.e-5        \n",
    "     if( is.vector(x) ) x = cbind( rep(1, length(x)), x ) \n",
    "     \n",
    "     n = nrow(x)       \n",
    "     if(any(x[,1]!=1))  x = cbind( rep(1, n), x )     \n",
    "      \n",
    "     p = ncol(x) \n",
    "     b = numeric(p)\n",
    "     t.x = t(x)\n",
    "\n",
    "     repeat\n",
    "     {\n",
    "       if(n>p) b.new = b.update.high(t.x, x, y, b, lambda)\n",
    "       else b.new = b.update.wide(t.x, x, y, b, lambda)\n",
    "       \n",
    "       diff = mean( abs(b.new - b) )\n",
    "       b = b.new;\n",
    "\n",
    "       if(diff < 1.e-8) break;\n",
    "     }\n",
    "\n",
    "     logit.fitted = as.vector(x %*% b)\n",
    "     pi1 = plogis(logit.fitted)\n",
    "     log.likelihood = sum( log(pi1[y==1]) ) + sum( log(1-pi1[y==0]) );\n",
    "     log.likelihood = log.likelihood/n;\n",
    "\n",
    "     list(b=b, logit.fitted=logit.fitted, log.likelihood=log.likelihood);\n",
    "   }\n",
    "   \n",
    "############ estimation.R #########################################\n",
    "##################################################### \n",
    "    \n",
    "   r.conditional.bernoulli = function(pi1, k)    #gamma: the log odds ratio, k: number of 1s.\n",
    "   {     \n",
    "     n = length(pi1)\n",
    "     repeat\n",
    "     {\n",
    "       yy = rbinom(n, 1, pi1)\n",
    "       if(sum(yy)==k) return(yy)\n",
    "     }\n",
    "   }\n",
    "\n",
    "   b0.estimate = function(offset1, k)\n",
    "   {               \n",
    "      func = function(b0)     #increasing function of b0\n",
    "      {         \n",
    "         temp = b0 + offset1\n",
    "         sum(plogis(temp)) - k         \n",
    "      }\n",
    "      \n",
    "      lower = 0\n",
    "      f0 = func(lower)\n",
    "      if(f0<0) factor1 = 1\n",
    "      else factor1 = -1\n",
    "      \n",
    "      repeat\n",
    "      {\n",
    "         upper = lower + factor1\n",
    "         f1 = func(upper)  \n",
    "         if(f0*f1<0) break\n",
    "         lower = upper\n",
    "         factor1 = factor1*2\n",
    "      }   \n",
    "     \n",
    "      if(factor1 > 0) b0 = uniroot(func, lower=lower, upper=upper, tol = 100*.Machine$double.eps^0.25)$root\n",
    "      else b0 = uniroot(func, lower=upper, upper=lower, tol = 100*.Machine$double.eps^0.25)$root\n",
    "      plogis( b0 + offset1)      \n",
    "   }   \n",
    "          \n",
    "################################################################################################\n",
    "  likelihood.one = function(x, y, localFDR.value, theta)\n",
    "  {   \n",
    "      k = sum(y)\n",
    "      p = ncol(x)\n",
    "      gene.selected = (runif(p)<1-localFDR.value[,1])\n",
    "      \n",
    "      b = rnorm(p, mean=0, sd=theta) \n",
    "      sign1 = ifelse(runif(p)<localFDR.value[,2], 1, -1) \n",
    "      b = abs(b) * sign1     \n",
    "\n",
    "      gamma1 = as.vector(x[, gene.selected] %*% b[gene.selected])\n",
    "      pi1 = b0.estimate(gamma1, k) \n",
    "                          #plot(y, pi1)\n",
    "      mean1 = sum(pi1)\n",
    "      sd1 = sqrt(sum(pi1*(1-pi1)))\n",
    "      denominator = pnorm(k+.5, mean1, sd1) - pnorm(k-.5, mean1, sd1)  #normal approximation\n",
    "      numerator = prod(dbinom(y,1,pi1)) \n",
    "       \n",
    "      numerator / denominator \n",
    "    }    \n",
    "       \n",
    "   \n",
    "    model.estimation = function(x, y, n.simu=5000)\n",
    "    {            \n",
    "       localFDR.value = as.matrix(read.dta(\"localFDR_value.dta\"))  \n",
    "       \n",
    "       k = sum(y)   \n",
    "       func = function(log.theta) \n",
    "       { \n",
    "         theta = exp(log.theta)\n",
    "         value = mean( replicate(n.simu, likelihood.one(x, y, localFDR.value, theta)) )\n",
    "         value = value / theta^.333 \n",
    "         print(c(theta, value))\n",
    "         value\n",
    "       }\n",
    "       result = optimize(f=func, lower=log(1.e-6), upper = log(1), maximum = TRUE) \n",
    "       theta = exp(result$maximum)\n",
    "       \n",
    "       print(\"The theta estimate is\"); print(theta)         \n",
    "       dump('theta',\"theta_estimate_dumped\")\n",
    "       theta           \n",
    "   }  \n",
    "   \n",
    "   \n",
    "############# Prediction.R ###################################################\n",
    "##################################################################\n",
    "  bootstrap.draw = function(x, k, localFDR.value, theta)\n",
    "  {  \n",
    "      p = ncol(x)\n",
    "      gene.selected = (runif(p)<1-localFDR.value[,1])\n",
    "      \n",
    "      b = rnorm(p, mean=0, sd=theta) \n",
    "      sign1 = ifelse(runif(p)<localFDR.value[,2], 1, -1) \n",
    "      b = abs(b) * sign1     \n",
    "\n",
    "      gamma1 = as.vector(x[, gene.selected] %*% b[gene.selected])\n",
    "      pi1 = (glm(y~1+offset(gamma1), family=binomial, epsilon=1.e-6))$fitted.values\n",
    "                           \n",
    "      yy = r.conditional.bernoulli(pi1, k)\n",
    "      list(pi1=pi1, y=yy)\n",
    "   }\n",
    "# For Prediction\n",
    "   pena.logit = function(x, y, q, tau)\n",
    "   {        \n",
    "      raw.pvalues = raw.pvalues.cal.one(x, y)[,1]\n",
    "      r1 = order(raw.pvalues)[1:q]  \n",
    "\n",
    "      model = logistic.penalized(x[,r1], y, lambda=1/tau^2)\n",
    "      list(b=model$b, fitted=plogis(model$logit.fitted), gene.selected=r1)\n",
    "   }\n",
    "     \n",
    "   prediction.error.make = function(x, y, q, n.simu)\n",
    "   {\n",
    "      localFDR.value = as.matrix(read.dta(\"localFDR_value.dta\"))     \n",
    "      source(\"theta_estimate_dumped\")\n",
    "\n",
    "      k = sum(y)\n",
    "      result = numeric(n.simu)\n",
    "      function(log.tau)\n",
    "      {\n",
    "        tau = exp(log.tau)\n",
    "        risk = 0\n",
    "        for(i in 1:n.simu)\n",
    "        {\n",
    "          obj = bootstrap.draw(x, k, localFDR.value, theta)  #conditional draw\n",
    "          pi1.fitted = pena.logit(x, obj$y, q, tau)$fitted  \n",
    "          result[i] = mean((obj$pi1- pi1.fitted)^2) + mean(obj$pi1*(1-obj$pi1)) \n",
    "        }\n",
    "        result =  mean(result)\n",
    "        print(c(tau, result))\n",
    "        result\n",
    "     }\n",
    "   }  \n",
    "       \n",
    "   bootstrap.prediction = function(x, y, q)  #penalized logistic regression\n",
    "   { \n",
    "     n.simu=1000       #simulation sample size\n",
    "     prediction.error = prediction.error.make(x, y, q, n.simu)          \n",
    "  \n",
    "     obj = optimize(prediction.error, c(log(.001), log(5)), maximum=FALSE)   #returns the optimal penalty\n",
    "     \n",
    "     print(c(\"The optimal tau is \", exp(obj$minimum)))\n",
    "     print(c(\"The prediction error is \", obj$objective)) \n",
    "   }\n",
    "  \n",
    "   \n",
    "############ Misselaneous #####################################\n",
    "###############################################\n",
    "  CV.pred = function(x, y, q, tau)\n",
    "  {\n",
    "     fitted.cross = numeric(length(y))\n",
    "\n",
    "     for(i in 1:length(y))\n",
    "     {\n",
    "       result = pena.logit(x[-i,], y[-i], q, tau) \n",
    "       print(result)          \n",
    "   \n",
    "       xx = x[i, result$gene.selected]   \n",
    "       xx = c(1, xx)  \n",
    "       fitted.cross[i] = plogis(sum(xx * result$b))\n",
    "       print(c(i, y[i], fitted.cross[i]))\n",
    "    }   \n",
    "   \n",
    "    plot(y, fitted.cross)\n",
    "  \n",
    "    print(mean((y-fitted.cross)^2)) \n",
    "    fitted.cross  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Load  data\n",
    "\n",
    "We preprocess the data to be the same as used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Train</th><td>38  </td><td>7129</td></tr>\n",
       "\t<tr><th scope=row>Test</th><td>34  </td><td>7129</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "\tTrain & 38   & 7129\\\\\n",
       "\tTest & 34   & 7129\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Train | 38   | 7129 | \n",
       "| Test | 34   | 7129 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      [,1] [,2]\n",
       "Train 38   7129\n",
       "Test  34   7129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Train</th><th scope=col>Test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>ALL</th><td>27</td><td>20</td></tr>\n",
       "\t<tr><th scope=row>AML</th><td>11</td><td>14</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Train & Test\\\\\n",
       "\\hline\n",
       "\tALL & 27 & 20\\\\\n",
       "\tAML & 11 & 14\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Train | Test | \n",
       "|---|---|\n",
       "| ALL | 27 | 20 | \n",
       "| AML | 11 | 14 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Train Test\n",
       "ALL 27    20  \n",
       "AML 11    14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Train</th><td>38  </td><td>3051</td></tr>\n",
       "\t<tr><th scope=row>Test</th><td>34  </td><td>3051</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "\tTrain & 38   & 3051\\\\\n",
       "\tTest & 34   & 3051\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Train | 38   | 3051 | \n",
       "| Test | 34   | 3051 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      [,1] [,2]\n",
       "Train 38   3051\n",
       "Test  34   3051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Train</th><th scope=col>Test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>ALL</th><td>27</td><td>20</td></tr>\n",
       "\t<tr><th scope=row>AML</th><td>11</td><td>14</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Train & Test\\\\\n",
       "\\hline\n",
       "\tALL & 27 & 20\\\\\n",
       "\tAML & 11 & 14\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Train | Test | \n",
       "|---|---|\n",
       "| ALL | 27 | 20 | \n",
       "| AML | 11 | 14 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Train Test\n",
       "ALL 27    20  \n",
       "AML 11    14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Most code is commented in this cell since it is unnecessary and time-consuming to run it everytime.\n",
    "# options(repos='http://cran.rstudio.com/') \n",
    "# source(\"http://bioconductor.org/biocLite.R\")\n",
    "# biocLite(\"golubEsets\")\n",
    "suppressMessages(library(golubEsets))\n",
    "suppressMessages(library(bnlearn))\n",
    "#Training data predictor and response\n",
    "data(Golub_Train)\n",
    "golub_train_p = t(exprs(Golub_Train))\n",
    "golub_train_r =pData(Golub_Train)[, \"ALL.AML\"]\n",
    "#Testing data predictor\n",
    "data(Golub_Test)\n",
    "golub_test_p = t(exprs(Golub_Test))\n",
    "golub_test_r = pData(Golub_Test)[, \"ALL.AML\"]\n",
    "#Show summary\n",
    "rbind(Train = dim(golub_train_p), Test = dim(golub_test_p))\n",
    "cbind(Train = table(golub_train_r),Test = table(golub_test_r))\n",
    "# Thresholding\n",
    "golub_train_pp = golub_train_p\n",
    "golub_train_pp[golub_train_pp<100] = 100\n",
    "golub_train_pp[golub_train_pp>16000] = 16000\n",
    "\n",
    "# Filtering\n",
    "golub_filter = function(x, r = 5, d=500){\n",
    "    minval = min(x)\n",
    "    maxval = max(x)\n",
    "    (maxval/minval>r)&&(maxval-minval>d)\n",
    "}\n",
    "index = apply(golub_train_pp, 2, golub_filter)\n",
    "golub_index = (1:7129)[index]\n",
    "golub_train_pp = golub_train_pp[, golub_index]\n",
    "\n",
    "golub_test_pp = golub_test_p\n",
    "golub_test_pp[golub_test_pp<100] = 100\n",
    "golub_test_pp[golub_test_pp>16000] = 16000\n",
    "golub_test_pp = golub_test_pp[, golub_index]\n",
    "\n",
    "# Log Transformation\n",
    "golub_train_p_trans = log10(golub_train_pp)\n",
    "golub_test_p_trans = log10(golub_test_pp)\n",
    "\n",
    "# Normalization\n",
    "train_m = colMeans(golub_train_p_trans)\n",
    "train_sd = apply(golub_train_p_trans, 2, sd)\n",
    "golub_train_p_trans = t((t(golub_train_p_trans)-train_m)/train_sd)\n",
    "golub_test_p_trans  = t((t(golub_test_p_trans)-train_m)/train_sd)\n",
    "rbind(Train = dim(golub_train_p_trans), Test = dim(golub_test_p_trans))\n",
    "cbind(Train = table(golub_train_r),Test = table(golub_test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "localFDR.cal(golub_train_p_trans, golub_train_r, v=100)\n",
    "#tau = model.estimation(golub_train_p_trans, golub_train_r)\n",
    "#bootstrap.prediction(golub_train_p_trans,  golub_train_r, q=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "###### Cervical Data set #################################\n",
    "# Read data\n",
    "   obj = read.data()\n",
    "   x = obj$x\n",
    "   y = obj$y\n",
    "   rm(obj)\n",
    "\n",
    "########################################\n",
    "\n",
    "   localFDR.cal(, golub_train_r, v=100)      #see Liao et al, Bioinformatics 20, 2694-2701\n",
    "   \n",
    "   tau = model.estimation(x, y)\n",
    "########################################\n",
    "# Prediction Error  \n",
    "   bootstrap.prediction(x, y, q=20)\n",
    "   \n",
    "###applying to test dataset ##########################################\n",
    "   result =  pena.logit(x, y, q=20, tau= .628)\n",
    "   \n",
    "########## applying to test dataset #################################\n",
    "\n",
    " read.data = function()\n",
    "  {\n",
    "    dataset = read.dta(\"golub_test.dta\")\n",
    "    y = dataset[,1]\n",
    "    x = as.matrix(dataset[,-1])\n",
    "    print(dim(x))\n",
    "\n",
    "    dimnames(x) = NULL \n",
    "    print(dim(x))\n",
    "    list(x=x, y=y)\n",
    "  }\n",
    "\n",
    "\n",
    "# Read data\n",
    "   dataset.test = read.data()\n",
    "   x = dataset.test$x\n",
    "   y = dataset.test$y\n",
    "   rm(dataset.test)\n",
    "\n",
    "   b = result$b\n",
    "   r1 = result$gene.selected\n",
    "   xx = x[, r1]\n",
    "   xx = cbind(rep(1, nrow(xx)), xx)\n",
    "   temp = as.vector(xx %*% b) + log(20/14) - log(27/11)\n",
    "   \n",
    "   fitted = plogis(temp)\n",
    "   \n",
    "   plot(y, fitted)\n",
    "  \n",
    "   mean((y-fitted)^2) \n",
    "  \n",
    "  ratio = sum(y)/sum(fitted)\n",
    "  fitted_adjusted = fitted*ratio\n",
    "  plot(y, fitted_adjusted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
